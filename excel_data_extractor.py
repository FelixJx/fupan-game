#!/usr/bin/env python3
"""
Excelæ•°æ®æå–å™¨ - åŸºäºåƒç‰›å“¥å¤ç›˜æ–¹æ³•è®º
ä»é‡å¯äººç”Ÿè®¡åˆ’è¡¨2.xlsxæå–é€‚åˆå¤ç›˜ä½“ç³»çš„æ•°æ®
"""

import pandas as pd
import os
import json
import numpy as np
from datetime import datetime, timedelta
import logging
from pathlib import Path
import sqlite3

class ExcelDataExtractor:
    """Excelæ•°æ®æå–å™¨ - ä¸“ä¸ºåƒç‰›å“¥å¤ç›˜ä½“ç³»è®¾è®¡"""
    
    def __init__(self, excel_file_path, db_path="fuPan_game.db"):
        self.excel_file_path = excel_file_path
        self.db_path = db_path
        self.extraction_config = self._load_extraction_config()
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_extraction_database()
    
    def _load_extraction_config(self):
        """é…ç½®åƒç‰›å“¥å…­æ­¥å¤ç›˜æ³•çš„æ•°æ®æå–è§„åˆ™"""
        return {
            # 1ï¸âƒ£ å¸‚åœºé¸Ÿç°æ•°æ®
            "market_overview": {
                "sheets": ["æ•´ä½“å¸‚åœºæ•°æ®", "æƒ…ç»ªå‘¨æœŸå›¾"],
                "key_fields": ["æ—¥æœŸ", "æ¶¨åœæ•°", "è·Œåœæ•°", "è¿æ¿æ•°", "é«˜åº¦æ¿é«˜åº¦", "ç‚¸æ¿ç‡", 
                             "æˆäº¤é‡", "ç ¸ç›˜ç³»æ•°", "æŒ£é’±æ•ˆåº”", "æœ€é«˜ä¸ªè‚¡"]
            },
            
            # 2ï¸âƒ£ é£é™©æ‰«ææ•°æ®
            "risk_scan": {
                "sheets": ["æ•´ä½“å¸‚åœºæ•°æ®", "æƒ…ç»ªå‘¨æœŸå›¾"],
                "key_fields": ["è·Œåœæ•°", "ç‚¸æ¿æ•°", "ç‚¸æ¿ç‡", "ç ¸ç›˜ç³»æ•°"]
            },
            
            # 3ï¸âƒ£ æœºä¼šæ‰«ææ•°æ®
            "opportunity_scan": {
                "sheets": ["5æ—¥è¿æ‰³æ¢¯é˜Ÿ", "é¢˜ææ¢¯é˜Ÿ", "é¢˜æé¢†æ¶¨è‚¡", "æ¶¨åœä¿¡æ¯"],
                "key_fields": ["è¿æ¿æ•°", "é¢˜æè½®åŠ¨", "æ¶¨åœåŸå› ", "æœ€é«˜ä¸ªè‚¡"]
            },
            
            # 4ï¸âƒ£ èµ„é‡‘éªŒè¯æ•°æ®
            "capital_verification": {
                "sheets": ["æˆäº¤é¢ç¯æ¯”è‚¡", "å¤§å•å‡€é¢è‚¡å†å²", "æ¸¸èµ„æ–¹å‘", "é¾™è™æ¦œ"],
                "key_fields": ["æˆäº¤é¢", "å¤§å•å‡€é¢", "æ¸¸èµ„æ–¹å‘", "é¾™è™æ¦œèµ„é‡‘"]
            },
            
            # 5ï¸âƒ£ é€»è¾‘æ ¸å¯¹æ•°æ®
            "logic_check": {
                "sheets": ["æ¯æ—¥æ–°é—»æ¶ˆæ¯é¢", "æ¶¨åœåŸå› ", "é¢˜æè½®åŠ¨æ•°æ®é›†"],
                "key_fields": ["æ–°é—»", "æ¶¨åœåŸå› ", "é¢˜æè½®åŠ¨"]
            },
            
            # 6ï¸âƒ£ æ ‡è®°åˆ†ç»„æ•°æ®
            "portfolio_management": {
                "sheets": ["è¡Œä¸šé¾™å¤´", "AIæ¨¡å‹ç²¾é€‰ä¼˜è´¨è‚¡", "æ¸¸èµ„ç›®å½•"],
                "key_fields": ["è¡Œä¸šé¾™å¤´", "ä¼˜è´¨è‚¡", "æ¸¸èµ„"]
            }
        }
    
    def _init_extraction_database(self):
        """åˆå§‹åŒ–æ•°æ®æå–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæå–æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS extracted_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL,
                data_type TEXT NOT NULL,
                sheet_name TEXT NOT NULL,
                data_content TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºæ¯æ—¥æ±‡æ€»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_summary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL UNIQUE,
                market_overview TEXT,
                risk_scan TEXT,
                opportunity_scan TEXT,
                capital_verification TEXT,
                logic_check TEXT,
                portfolio_management TEXT,
                extraction_status TEXT DEFAULT 'pending',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def extract_all_data(self, target_date=None):
        """æå–æ‰€æœ‰é€‚åˆå¤ç›˜ä½“ç³»çš„æ•°æ®"""
        if not target_date:
            target_date = datetime.now().strftime('%Y-%m-%d')
        
        self.logger.info(f"ğŸ® å¼€å§‹æå– {target_date} çš„å¤ç›˜æ•°æ®...")
        
        try:
            # è¯»å–Excelæ–‡ä»¶
            excel_data = pd.ExcelFile(self.excel_file_path)
            extraction_result = {
                "date": target_date,
                "extraction_time": datetime.now().isoformat(),
                "data": {}
            }
            
            # æŒ‰åƒç‰›å“¥å…­æ­¥æ³•æå–æ•°æ®
            for step_name, config in self.extraction_config.items():
                step_data = self._extract_step_data(excel_data, step_name, config, target_date)
                extraction_result["data"][step_name] = step_data
                
                self.logger.info(f"âœ… {step_name} æ•°æ®æå–å®Œæˆ")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self._save_extraction_result(extraction_result)
            
            self.logger.info(f"ğŸ¯ {target_date} æ•°æ®æå–å®Œæˆï¼")
            return extraction_result
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
            return None
    
    def _extract_step_data(self, excel_data, step_name, config, target_date):
        """æå–å•ä¸ªæ­¥éª¤çš„æ•°æ®"""
        step_data = {}
        
        for sheet_name in config["sheets"]:
            if sheet_name in excel_data.sheet_names:
                try:
                    df = pd.read_excel(self.excel_file_path, sheet_name=sheet_name)
                    
                    # æ ¹æ®æ­¥éª¤ç±»å‹æå–ç›¸åº”æ•°æ®
                    extracted = self._extract_sheet_data(df, sheet_name, step_name, target_date)
                    if extracted:
                        step_data[sheet_name] = extracted
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} æå–å¤±è´¥: {e}")
        
        return step_data
    
    def _extract_sheet_data(self, df, sheet_name, step_name, target_date):
        """ä»å…·ä½“å·¥ä½œè¡¨æå–æ•°æ®"""
        
        if sheet_name == "æ•´ä½“å¸‚åœºæ•°æ®":
            return self._extract_market_data(df, target_date)
        
        elif sheet_name == "æƒ…ç»ªå‘¨æœŸå›¾":
            return self._extract_emotion_data(df, target_date)
        
        elif sheet_name == "5æ—¥è¿æ‰³æ¢¯é˜Ÿ":
            return self._extract_continuous_board_data(df, target_date)
        
        elif sheet_name == "é¢˜ææ¢¯é˜Ÿ":
            return self._extract_theme_data(df, target_date)
        
        elif sheet_name == "æ¶¨åœä¿¡æ¯":
            return self._extract_limit_up_data(df, target_date)
        
        elif sheet_name == "é¾™è™æ¦œ":
            return self._extract_dragon_tiger_data(df, target_date)
        
        elif sheet_name == "æ¯æ—¥æ–°é—»æ¶ˆæ¯é¢":
            return self._extract_news_data(df, target_date)
        
        # å…¶ä»–å·¥ä½œè¡¨çš„é€šç”¨æå–
        return self._extract_generic_data(df, sheet_name, target_date)
    
    def _extract_market_data(self, df, target_date):
        """æå–æ•´ä½“å¸‚åœºæ•°æ® - å¸‚åœºé¸Ÿç°"""
        try:
            # æŸ¥æ‰¾æœ€æ–°æ—¥æœŸæ•°æ®
            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
                latest_data = df.dropna(subset=['æ—¥æœŸ']).tail(1)
                
                if not latest_data.empty:
                    # è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                    market_data = {
                        "date": latest_data['æ—¥æœŸ'].iloc[0].strftime('%Y-%m-%d'),
                        "limit_up_count": int(latest_data.get('æ¶¨åœæ•°', [0]).iloc[0]) if pd.notna(latest_data.get('æ¶¨åœæ•°', [0]).iloc[0]) else 0,
                        "limit_down_count": int(latest_data.get('è·Œåœæ•°', [0]).iloc[0]) if pd.notna(latest_data.get('è·Œåœæ•°', [0]).iloc[0]) else 0,
                        "continuous_boards": int(latest_data.get('è¿æ¿æ•°', [0]).iloc[0]) if pd.notna(latest_data.get('è¿æ¿æ•°', [0]).iloc[0]) else 0,
                        "highest_board": int(latest_data.get('é«˜åº¦æ¿é«˜åº¦', [0]).iloc[0]) if pd.notna(latest_data.get('é«˜åº¦æ¿é«˜åº¦', [0]).iloc[0]) else 0,
                        "blow_up_count": int(latest_data.get('ç‚¸æ¿æ•°', [0]).iloc[0]) if pd.notna(latest_data.get('ç‚¸æ¿æ•°', [0]).iloc[0]) else 0,
                        "blow_up_rate": float(latest_data.get('ç‚¸æ¿ç‡', [0]).iloc[0]) if pd.notna(latest_data.get('ç‚¸æ¿ç‡', [0]).iloc[0]) else 0.0,
                        "volume": float(latest_data.get('æˆäº¤é‡', [0]).iloc[0]) if pd.notna(latest_data.get('æˆäº¤é‡', [0]).iloc[0]) else 0.0,
                        "top_stock": str(latest_data.get('æœ€é«˜ä¸ªè‚¡', ['æ— ']).iloc[0]) if pd.notna(latest_data.get('æœ€é«˜ä¸ªè‚¡', ['æ— ']).iloc[0]) else "æ— ",
                        "up_count": int(latest_data.get('ä¸Šæ¶¨æ•°', [0]).iloc[0]) if pd.notna(latest_data.get('ä¸Šæ¶¨æ•°', [0]).iloc[0]) else 0,
                        "down_count": int(latest_data.get('ä¸‹è·Œæ•°', [0]).iloc[0]) if pd.notna(latest_data.get('ä¸‹è·Œæ•°', [0]).iloc[0]) else 0
                    }
                    
                    # åƒç‰›å“¥åˆ†æ
                    market_data["qianniu_analysis"] = self._analyze_market_sentiment(market_data)
                    
                    return market_data
        except Exception as e:
            self.logger.error(f"æ•´ä½“å¸‚åœºæ•°æ®æå–å¤±è´¥: {e}")
        
        return None
    
    def _extract_emotion_data(self, df, target_date):
        """æå–æƒ…ç»ªå‘¨æœŸæ•°æ® - é£é™©å—…è§‰"""
        try:
            if len(df) > 0:
                latest_data = df.tail(1)
                
                emotion_data = {
                    "blow_up_ratio": float(latest_data.get('ç ¸ç›˜ç³»æ•°1', [0]).iloc[0]) if 'ç ¸ç›˜ç³»æ•°1' in df.columns and pd.notna(latest_data.get('ç ¸ç›˜ç³»æ•°1', [0]).iloc[0]) else 0.0,
                    "money_effect": int(latest_data.get('æŒ£é’±æ•ˆåº”', [0]).iloc[0]) if 'æŒ£é’±æ•ˆåº”' in df.columns and pd.notna(latest_data.get('æŒ£é’±æ•ˆåº”', [0]).iloc[0]) else 0,
                    "emotion_cycle": "åˆ†æä¸­",
                    "risk_level": "ä¸­ç­‰"
                }
                
                # åƒç‰›å“¥æƒ…ç»ªåˆ†æ
                emotion_data["qianniu_emotion"] = self._analyze_emotion_cycle(emotion_data)
                
                return emotion_data
        except Exception as e:
            self.logger.error(f"æƒ…ç»ªå‘¨æœŸæ•°æ®æå–å¤±è´¥: {e}")
        
        return None
    
    def _extract_continuous_board_data(self, df, target_date):
        """æå–è¿æ¿æ¢¯é˜Ÿæ•°æ® - æœºä¼šæ•æ‰"""
        try:
            board_data = {
                "date": target_date,
                "boards": {},
                "themes": [],
                "leading_stocks": []
            }
            
            # åˆ†æè¿æ¿æ¢¯é˜Ÿç»“æ„
            for col in df.columns:
                if 'Unnamed' not in str(col) and df[col].notna().any():
                    # æå–è¿æ¿ä¿¡æ¯
                    valid_data = df[col].dropna().astype(str)
                    if len(valid_data) > 0:
                        board_data["themes"].extend([x for x in valid_data.tolist()[:5] if 'æ¿' in x])
            
            # åƒç‰›å“¥è¿æ¿åˆ†æ
            board_data["qianniu_board_analysis"] = self._analyze_board_structure(board_data)
            
            return board_data
        except Exception as e:
            self.logger.error(f"è¿æ¿æ•°æ®æå–å¤±è´¥: {e}")
        
        return None
    
    def _extract_theme_data(self, df, target_date):
        """æå–é¢˜ææ¢¯é˜Ÿæ•°æ® - é€»è¾‘åˆ†æ"""
        try:
            theme_data = {
                "date": target_date,
                "hot_themes": [],
                "theme_structure": {},
                "leading_themes": []
            }
            
            # æå–é¢˜æä¿¡æ¯
            if len(df) > 0:
                for idx, row in df.head(5).iterrows():
                    for col in df.columns:
                        cell_value = str(row[col])
                        if pd.notna(row[col]) and len(cell_value) > 2:
                            # è¯†åˆ«é¢˜æå…³é”®è¯
                            if any(keyword in cell_value for keyword in ['åŒ»è¯', 'æ–°èƒ½æº', 'äººå·¥æ™ºèƒ½', 'æœºå™¨äºº', 'å…‰ä¼']):
                                if cell_value not in theme_data["hot_themes"]:
                                    theme_data["hot_themes"].append(cell_value)
            
            # åƒç‰›å“¥é¢˜æåˆ†æ
            theme_data["qianniu_theme_analysis"] = self._analyze_theme_rotation(theme_data)
            
            return theme_data
        except Exception as e:
            self.logger.error(f"é¢˜ææ•°æ®æå–å¤±è´¥: {e}")
        
        return None
    
    def _extract_limit_up_data(self, df, target_date):
        """æå–æ¶¨åœæ•°æ®"""
        try:
            return {
                "date": target_date,
                "limit_up_stocks": df.head(20).to_dict('records') if len(df) > 0 else [],
                "count": len(df)
            }
        except Exception as e:
            self.logger.error(f"æ¶¨åœæ•°æ®æå–å¤±è´¥: {e}")
        return None
    
    def _extract_dragon_tiger_data(self, df, target_date):
        """æå–é¾™è™æ¦œæ•°æ® - èµ„é‡‘éªŒè¯"""
        try:
            return {
                "date": target_date,
                "dragon_tiger_list": df.head(10).to_dict('records') if len(df) > 0 else [],
                "hot_money_direction": "åˆ†æä¸­"
            }
        except Exception as e:
            self.logger.error(f"é¾™è™æ¦œæ•°æ®æå–å¤±è´¥: {e}")
        return None
    
    def _extract_news_data(self, df, target_date):
        """æå–æ–°é—»æ•°æ® - é€»è¾‘æ ¸å¯¹"""
        try:
            return {
                "date": target_date,
                "news_summary": df.head(10).to_dict('records') if len(df) > 0 else [],
                "hot_topics": []
            }
        except Exception as e:
            self.logger.error(f"æ–°é—»æ•°æ®æå–å¤±è´¥: {e}")
        return None
    
    def _extract_generic_data(self, df, sheet_name, target_date):
        """é€šç”¨æ•°æ®æå–"""
        try:
            return {
                "sheet_name": sheet_name,
                "date": target_date,
                "data_summary": f"å…±{len(df)}è¡Œæ•°æ®",
                "sample_data": df.head(3).to_dict('records') if len(df) > 0 else []
            }
        except Exception as e:
            self.logger.error(f"{sheet_name}é€šç”¨æ•°æ®æå–å¤±è´¥: {e}")
        return None
    
    def _analyze_market_sentiment(self, market_data):
        """åƒç‰›å“¥å¸‚åœºæƒ…ç»ªåˆ†æ"""
        limit_up = market_data.get('limit_up_count', 0)
        limit_down = market_data.get('limit_down_count', 0)
        blow_up_rate = market_data.get('blow_up_rate', 0)
        
        if limit_up > 80 and blow_up_rate < 0.3:
            return "å¼ºåŠ¿å¸‚åœºï¼Œæƒ…ç»ªé«˜æ¶¨ï¼Œæ³¨æ„é«˜ä½é£é™©"
        elif limit_up > 50 and blow_up_rate < 0.4:
            return "åå¼ºå¸‚åœºï¼Œæƒ…ç»ªè¾ƒå¥½ï¼Œå¯ç§¯æå‚ä¸"
        elif limit_up < 30 or blow_up_rate > 0.5:
            return "å¼±åŠ¿å¸‚åœºï¼Œæƒ…ç»ªä½è¿·ï¼Œä»¥é˜²å®ˆä¸ºä¸»"
        else:
            return "éœ‡è¡å¸‚åœºï¼Œæƒ…ç»ªä¸­æ€§ï¼Œç²¾é€‰ä¸ªè‚¡"
    
    def _analyze_emotion_cycle(self, emotion_data):
        """åƒç‰›å“¥æƒ…ç»ªå‘¨æœŸåˆ†æ"""
        blow_up_ratio = emotion_data.get('blow_up_ratio', 0)
        money_effect = emotion_data.get('money_effect', 0)
        
        if blow_up_ratio > 2 and money_effect < 3:
            return "æƒ…ç»ªé€€æ½®æœŸï¼Œé«˜æ ‡é¿é™©ï¼Œç­‰å¾…æ–°å‘¨æœŸ"
        elif blow_up_ratio < 1 and money_effect > 5:
            return "æƒ…ç»ªå¯åŠ¨æœŸï¼Œå¯å…³æ³¨ä½ä½æœºä¼š"
        else:
            return "æƒ…ç»ªä¸­ç»§æœŸï¼Œä¿æŒè§‚å¯Ÿ"
    
    def _analyze_board_structure(self, board_data):
        """åƒç‰›å“¥è¿æ¿ç»“æ„åˆ†æ"""
        themes_count = len(board_data.get('themes', []))
        
        if themes_count > 10:
            return "è¿æ¿æ¢¯é˜Ÿå®Œæ•´ï¼Œæœ‰æŒç»­æ€§ï¼Œä¸»çº¿æ˜ç¡®"
        elif themes_count > 5:
            return "è¿æ¿æ¢¯é˜Ÿè¾ƒå¥½ï¼Œå¯å…³æ³¨é¾™å¤´"
        else:
            return "è¿æ¿æ¢¯é˜Ÿä¸å®Œæ•´ï¼Œè°¨æ…å‚ä¸"
    
    def _analyze_theme_rotation(self, theme_data):
        """åƒç‰›å“¥é¢˜æè½®åŠ¨åˆ†æ"""
        hot_themes = theme_data.get('hot_themes', [])
        
        medical_count = sum(1 for theme in hot_themes if 'åŒ»è¯' in theme)
        tech_count = sum(1 for theme in hot_themes if any(x in theme for x in ['äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'æœºå™¨äºº']))
        
        if medical_count > tech_count:
            return "åŒ»è¯é¢˜æå ä¸»å¯¼ï¼Œé˜²å¾¡æ€§è¾ƒå¼º"
        elif tech_count > medical_count:
            return "ç§‘æŠ€é¢˜ææ´»è·ƒï¼Œæˆé•¿æ€§è¾ƒå¥½"
        else:
            return "é¢˜æè½®åŠ¨å‡è¡¡ï¼Œå¤šå…ƒåŒ–æ ¼å±€"
    
    def _save_extraction_result(self, result):
        """ä¿å­˜æå–ç»“æœåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        date = result["date"]
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        for data_type, data_content in result["data"].items():
            for sheet_name, sheet_data in data_content.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO extracted_data 
                    (date, data_type, sheet_name, data_content)
                    VALUES (?, ?, ?, ?)
                ''', (date, data_type, sheet_name, json.dumps(sheet_data, ensure_ascii=False)))
        
        # ä¿å­˜æ¯æ—¥æ±‡æ€»
        cursor.execute('''
            INSERT OR REPLACE INTO daily_summary 
            (date, market_overview, risk_scan, opportunity_scan, 
             capital_verification, logic_check, portfolio_management, extraction_status)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            date,
            json.dumps(result["data"].get("market_overview", {}), ensure_ascii=False),
            json.dumps(result["data"].get("risk_scan", {}), ensure_ascii=False),
            json.dumps(result["data"].get("opportunity_scan", {}), ensure_ascii=False),
            json.dumps(result["data"].get("capital_verification", {}), ensure_ascii=False),
            json.dumps(result["data"].get("logic_check", {}), ensure_ascii=False),
            json.dumps(result["data"].get("portfolio_management", {}), ensure_ascii=False),
            "completed"
        ))
        
        conn.commit()
        conn.close()
    
    def get_daily_data(self, date):
        """è·å–æŒ‡å®šæ—¥æœŸçš„å¤ç›˜æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM daily_summary WHERE date = ?', (date,))
        result = cursor.fetchone()
        
        conn.close()
        
        if result:
            return {
                "date": result[1],
                "market_overview": json.loads(result[2]) if result[2] else {},
                "risk_scan": json.loads(result[3]) if result[3] else {},
                "opportunity_scan": json.loads(result[4]) if result[4] else {},
                "capital_verification": json.loads(result[5]) if result[5] else {},
                "logic_check": json.loads(result[6]) if result[6] else {},
                "portfolio_management": json.loads(result[7]) if result[7] else {},
                "status": result[8]
            }
        
        return None

# è‡ªåŠ¨æ–‡æ¡£ç›‘æ§ç±»
class DocumentMonitor:
    """æ–‡æ¡£ç›‘æ§å™¨ - è‡ªåŠ¨æ£€æµ‹æ–°Excelæ–‡ä»¶"""
    
    def __init__(self, watch_directory, extractor_class=ExcelDataExtractor):
        self.watch_directory = Path(watch_directory)
        self.extractor_class = extractor_class
        self.processed_files = set()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def scan_for_new_files(self):
        """æ‰«ææ–°çš„Excelæ–‡ä»¶"""
        excel_files = list(self.watch_directory.glob("*.xlsx"))
        excel_files.extend(list(self.watch_directory.glob("*.xls")))
        
        new_files = []
        for file_path in excel_files:
            if file_path.name not in self.processed_files:
                new_files.append(file_path)
                self.processed_files.add(file_path.name)
        
        return new_files
    
    def process_new_files(self):
        """å¤„ç†æ–°å‘ç°çš„Excelæ–‡ä»¶"""
        new_files = self.scan_for_new_files()
        
        for file_path in new_files:
            self.logger.info(f"ğŸ” å‘ç°æ–°æ–‡ä»¶: {file_path.name}")
            
            try:
                # åˆ›å»ºæå–å™¨å¹¶æå–æ•°æ®
                extractor = self.extractor_class(str(file_path))
                result = extractor.extract_all_data()
                
                if result:
                    self.logger.info(f"âœ… {file_path.name} æ•°æ®æå–æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ {file_path.name} æ•°æ®æå–å¤±è´¥")
                    
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®æå–
    excel_file = "/Users/jx/Desktop/stock-agent3.0/é‡å¯äººç”Ÿè®¡åˆ’è¡¨2.xlsx"
    
    print("ğŸ® Excelæ•°æ®æå–å™¨æµ‹è¯•")
    print("åŸºäºåƒç‰›å“¥å¤ç›˜æ–¹æ³•è®º")
    print("=" * 50)
    
    if os.path.exists(excel_file):
        extractor = ExcelDataExtractor(excel_file)
        result = extractor.extract_all_data()
        
        if result:
            print("âœ… æ•°æ®æå–æˆåŠŸï¼")
            print(f"ğŸ“Š æå–æ—¥æœŸ: {result['date']}")
            print(f"ğŸ¯ æ•°æ®æ­¥éª¤: {list(result['data'].keys())}")
            
            # æ˜¾ç¤ºå¸‚åœºæ¦‚è§ˆæ ·æœ¬
            market_data = result['data'].get('market_overview', {})
            if market_data:
                print("\nğŸ“ˆ å¸‚åœºæ¦‚è§ˆæ ·æœ¬:")
                for sheet, data in market_data.items():
                    if data:
                        print(f"  {sheet}: {data.get('qianniu_analysis', 'æ•°æ®è·å–æˆåŠŸ')}")
        else:
            print("âŒ æ•°æ®æå–å¤±è´¥")
    else:
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")