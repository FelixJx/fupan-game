#!/usr/bin/env python3
"""
è‚¡ç¥¨å¤ç›˜æ¸¸æˆ - æ•°æ®æ¥å…¥å’ŒéªŒè¯ç³»ç»Ÿ
åŸºäºåƒç‰›å“¥å¤ç›˜æ–¹æ³•è®º
"""

import sqlite3
import pandas as pd
import akshare as ak
import tushare as ts
import requests
from datetime import datetime, timedelta
import json
import logging
import os
import time
import numpy as np
from excel_data_extractor import ExcelDataExtractor

class StockDataSystem:
    def __init__(self, db_path="fuPan_game.db", tushare_token=None):
        self.db_path = db_path
        self.excel_extractor = None
        
        # æ•°æ®æºé…ç½® - å…ˆåˆå§‹åŒ–
        self.data_sources = {
            'tushare': False,
            'akshare': True,
            'efinance': True,
            'eastmoney': True,
            'excel': False  # å°†æ ¹æ®æ£€æµ‹ç»“æœæ›´æ–°
        }
        
        # åˆå§‹åŒ–Tushare
        self.tushare_token = tushare_token or "b34d8920b99b43d48df7e792a4708a29f868feeee30d9c84b54bf065"
        self.ts_pro = None
        self._init_tushare()
        
        self.init_database()
        logging.basicConfig(level=logging.INFO)
        
        # è‡ªåŠ¨æ£€æµ‹Excelæ•°æ®æ–‡ä»¶
        self._detect_excel_data_source()
    
    def _init_tushare(self):
        """åˆå§‹åŒ–Tushare Pro API"""
        try:
            ts.set_token(self.tushare_token)
            self.ts_pro = ts.pro_api()
            # ç®€å•æµ‹è¯•è¿æ¥ï¼Œä¸æ‰§è¡Œå¤æ‚æŸ¥è¯¢é¿å…è¶…æ—¶
            logging.info("ğŸ“¡ Tushare Proå·²é…ç½®ï¼Œå°†åœ¨ä½¿ç”¨æ—¶éªŒè¯è¿æ¥")
            self.data_sources['tushare'] = True
        except Exception as e:
            logging.warning(f"âš ï¸ Tushare Proåˆå§‹åŒ–è­¦å‘Š: {e}")
            self.data_sources['tushare'] = False
            self.ts_pro = None
        
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å¤ç›˜è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS fuPan_records (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT NOT NULL,
                date TEXT NOT NULL,
                step1_market_overview TEXT,
                step2_risk_scan TEXT,
                step3_opportunity_scan TEXT,
                step4_capital_verification TEXT,
                step5_logic_check TEXT,
                step6_portfolio_management TEXT,
                predictions TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å¸‚åœºæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS market_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL,
                market_overview TEXT,
                top_gainers TEXT,
                top_losers TEXT,
                volume_leaders TEXT,
                sector_performance TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # è¯„åˆ†ç»“æœè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scoring_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                record_id INTEGER,
                prediction_accuracy_score REAL,
                analysis_depth_score REAL,
                total_score REAL,
                next_day_performance TEXT,
                scoring_date TEXT,
                FOREIGN KEY (record_id) REFERENCES fuPan_records (id)
            )
        ''')
        
        # å®æ—¶å¸‚åœºæ•°æ®è¡¨ï¼ˆTushareï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS realtime_market_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts_code TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                open REAL, high REAL, low REAL, close REAL,
                pre_close REAL, change REAL, pct_chg REAL,
                vol REAL, amount REAL,
                data_source TEXT DEFAULT 'tushare',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(ts_code, trade_date)
            )
        ''')
        
        # æ¿å—æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sector_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sector_name TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                sector_type TEXT, -- 'æ¦‚å¿µæ¿å—', 'è¡Œä¸šæ¿å—', 'åœ°åŸŸæ¿å—'
                change_percent REAL,
                volume_ratio REAL,
                leading_stocks TEXT,
                stock_count INTEGER,
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(sector_name, trade_date)
            )
        ''')
        
        # æ¶¨è·Œåœæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS limit_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts_code TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                name TEXT,
                close REAL,
                pct_chg REAL,
                limit_type TEXT, -- 'up' or 'down'
                reason TEXT,
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # äº‹ä»¶æ—¥å†è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS event_calendar (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_date TEXT NOT NULL,
                event_type TEXT, -- 'ä¼šè®®', 'æ”¿ç­–', 'è´¢æŠ¥'
                event_name TEXT,
                related_sectors TEXT,
                importance_level INTEGER,
                expected_impact TEXT,
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # èµ„é‡‘æµå‘æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS capital_flow (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ts_code TEXT,
                trade_date TEXT NOT NULL,
                buy_sm_vol REAL, -- å°å•ä¹°å…¥é‡
                buy_md_vol REAL, -- ä¸­å•ä¹°å…¥é‡
                buy_lg_vol REAL, -- å¤§å•ä¹°å…¥é‡
                buy_elg_vol REAL, -- ç‰¹å¤§å•ä¹°å…¥é‡
                sell_sm_vol REAL,
                sell_md_vol REAL, 
                sell_lg_vol REAL,
                sell_elg_vol REAL,
                net_mf_vol REAL, -- å‡€æµå…¥é‡
                data_source TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _detect_excel_data_source(self):
        """æ£€æµ‹Excelæ•°æ®æº"""
        # æ£€æµ‹å¸¸è§è·¯å¾„
        potential_paths = [
            "/Users/jx/Desktop/stock-agent3.0/é‡å¯äººç”Ÿè®¡åˆ’è¡¨2.xlsx",
            "./é‡å¯äººç”Ÿè®¡åˆ’è¡¨2.xlsx",
            "../é‡å¯äººç”Ÿè®¡åˆ’è¡¨2.xlsx"
        ]
        
        for path in potential_paths:
            if os.path.exists(path):
                try:
                    self.excel_extractor = ExcelDataExtractor(path, self.db_path)
                    logging.info(f"âœ… Excelæ•°æ®æºè¿æ¥æˆåŠŸ: {path}")
                    break
                except Exception as e:
                    logging.warning(f"âš ï¸ Excelæ•°æ®æºè¿æ¥å¤±è´¥ {path}: {e}")
        
        if not self.excel_extractor:
            logging.info("â„¹ï¸ æœªæ£€æµ‹åˆ°Excelæ•°æ®æºï¼Œå°†ä½¿ç”¨åœ¨çº¿æ•°æ®")
        else:
            self.data_sources['excel'] = True
    
    # ======================== Tushareæ•°æ®é‡‡é›†æ–¹æ³• ========================
    
    def get_tushare_market_overview(self, trade_date=None):
        """ä»Tushareè·å–å¸‚åœºæ¦‚è§ˆæ•°æ®"""
        if not self.ts_pro:
            return None
            
        if not trade_date:
            trade_date = datetime.now().strftime('%Y%m%d')
        else:
            # è½¬æ¢æ—¥æœŸæ ¼å¼ 2025-08-03 -> 20250803
            trade_date = trade_date.replace('-', '')
        
        try:
            # è·å–æŒ‡æ•°æ•°æ®
            indices_data = self.ts_pro.index_daily(trade_date=trade_date)
            
            # è·å–æ¶¨è·Œåœæ•°æ®
            limit_up_data = self.ts_pro.limit_list_d(trade_date=trade_date, limit_type='U')
            limit_down_data = self.ts_pro.limit_list_d(trade_date=trade_date, limit_type='D')
            
            # è·å–å¸‚åœºç»Ÿè®¡æ•°æ®
            market_stats = self.ts_pro.daily_basic(trade_date=trade_date)
            
            return {
                'indices': self._process_indices_data(indices_data),
                'limit_up_count': len(limit_up_data) if not limit_up_data.empty else 0,
                'limit_down_count': len(limit_down_data) if not limit_down_data.empty else 0,
                'market_stats': self._process_market_stats(market_stats),
                'data_source': 'tushare'
            }
            
        except Exception as e:
            logging.error(f"Tushareå¸‚åœºæ¦‚è§ˆæ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def get_tushare_limit_stocks(self, trade_date=None, limit_type='U'):
        """è·å–æ¶¨è·Œåœè‚¡ç¥¨æ•°æ®"""
        if not self.ts_pro:
            return []
            
        if not trade_date:
            trade_date = datetime.now().strftime('%Y%m%d')
        else:
            trade_date = trade_date.replace('-', '')
        
        try:
            limit_data = self.ts_pro.limit_list_d(trade_date=trade_date, limit_type=limit_type)
            
            if limit_data.empty:
                return []
            
            return [{
                'ts_code': row['ts_code'],
                'name': row['name'],
                'close': row['close'],
                'pct_chg': row['pct_chg'],
                'amount': row['amount'],
                'limit_type': 'up' if limit_type == 'U' else 'down',
                'data_source': 'tushare'
            } for _, row in limit_data.iterrows()]
            
        except Exception as e:
            logging.error(f"Tushareæ¶¨è·Œåœæ•°æ®è·å–å¤±è´¥: {e}")
            return []
    
    def get_tushare_sector_data(self, trade_date=None):
        """è·å–Tushareæ¿å—æ•°æ®"""
        if not self.ts_pro:
            return []
            
        if not trade_date:
            trade_date = datetime.now().strftime('%Y%m%d')
        else:
            trade_date = trade_date.replace('-', '')
        
        try:
            # è·å–æ¦‚å¿µæ¿å—æ•°æ®
            concept_data = self.ts_pro.concept_detail(trade_date=trade_date)
            
            # è·å–è¡Œä¸šæ¿å—æ•°æ®  
            industry_data = self.ts_pro.index_classify(level='L2', src='SW2021')
            
            sectors = []
            
            # å¤„ç†æ¦‚å¿µæ¿å—
            if not concept_data.empty:
                concept_grouped = concept_data.groupby('concept_name').agg({
                    'ts_code': 'count',
                    'close': 'mean',
                    'pct_chg': 'mean'
                }).reset_index()
                
                for _, row in concept_grouped.iterrows():
                    sectors.append({
                        'sector_name': row['concept_name'],
                        'sector_type': 'æ¦‚å¿µæ¿å—',
                        'stock_count': row['ts_code'],
                        'avg_change': row['pct_chg'],
                        'data_source': 'tushare'
                    })
            
            return sectors
            
        except Exception as e:
            logging.error(f"Tushareæ¿å—æ•°æ®è·å–å¤±è´¥: {e}")
            return []
    
    def get_tushare_capital_flow(self, trade_date=None, ts_code=None):
        """è·å–èµ„é‡‘æµå‘æ•°æ®"""
        if not self.ts_pro:
            return None
            
        if not trade_date:
            trade_date = datetime.now().strftime('%Y%m%d')
        else:
            trade_date = trade_date.replace('-', '')
        
        try:
            # è·å–èµ„é‡‘æµå‘æ•°æ®
            if ts_code:
                # è·å–ä¸ªè‚¡èµ„é‡‘æµå‘
                money_flow = self.ts_pro.moneyflow(ts_code=ts_code, trade_date=trade_date)
            else:
                # è·å–å¸‚åœºæ•´ä½“èµ„é‡‘æµå‘
                money_flow = self.ts_pro.moneyflow_hsgt(trade_date=trade_date)
            
            if money_flow.empty:
                return None
            
            return {
                'data': money_flow.to_dict('records'),
                'data_source': 'tushare'
            }
            
        except Exception as e:
            logging.error(f"Tushareèµ„é‡‘æµå‘æ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def _process_indices_data(self, indices_data):
        """å¤„ç†æŒ‡æ•°æ•°æ®"""
        if indices_data.empty:
            return {}
        
        indices = {}
        for _, row in indices_data.iterrows():
            ts_code = row['ts_code']
            if ts_code in ['000001.SH', '399001.SZ', '399006.SZ']:  # ä¸Šè¯ã€æ·±æˆæŒ‡ã€åˆ›ä¸šæ¿
                name_map = {
                    '000001.SH': 'ä¸Šè¯æŒ‡æ•°',
                    '399001.SZ': 'æ·±è¯æˆæŒ‡', 
                    '399006.SZ': 'åˆ›ä¸šæ¿æŒ‡'
                }
                indices[name_map[ts_code]] = {
                    'close': row['close'],
                    'change': row['change'],
                    'pct_chg': row['pct_chg'],
                    'vol': row['vol'],
                    'amount': row['amount']
                }
        
        return indices
    
    def _process_market_stats(self, market_stats):
        """å¤„ç†å¸‚åœºç»Ÿè®¡æ•°æ®"""
        if market_stats.empty:
            return {}
        
        # è®¡ç®—æ¶¨è·Œå®¶æ•°
        up_count = len(market_stats[market_stats['pct_chg'] > 0])
        down_count = len(market_stats[market_stats['pct_chg'] < 0])
        flat_count = len(market_stats[market_stats['pct_chg'] == 0])
        
        # è®¡ç®—å¹³å‡æ¶¨è·Œå¹…
        avg_pct_chg = market_stats['pct_chg'].mean()
        
        # è®¡ç®—æˆäº¤é¢
        total_amount = market_stats['amount'].sum()
        
        return {
            'up_count': up_count,
            'down_count': down_count,
            'flat_count': flat_count,
            'avg_pct_chg': avg_pct_chg,
            'total_amount': total_amount
        }
    
    # ======================== AKShareå¢å¼ºæ•°æ®é‡‡é›† ========================
    
    def get_akshare_enhanced_data(self, date_str=None):
        """è·å–AKShareå¢å¼ºæ•°æ®"""
        try:
            # å®æ—¶æ¦‚å¿µæ¿å—æ•°æ®
            concept_data = ak.stock_board_concept_name_em()
            
            # å®æ—¶è¡Œä¸šæ¿å—æ•°æ®
            industry_data = ak.stock_board_industry_name_em()
            
            # èµ„é‡‘æµå‘æ•°æ®
            fund_flow = ak.stock_individual_fund_flow_rank(indicator="ä»Šæ—¥")
            
            # é¾™è™æ¦œæ•°æ®
            lhb_data = ak.stock_lhb_detail_em(trade_date=date_str or datetime.now().strftime('%Y%m%d'))
            
            return {
                'concept_boards': concept_data.to_dict('records') if not concept_data.empty else [],
                'industry_boards': industry_data.to_dict('records') if not industry_data.empty else [],
                'fund_flow': fund_flow.to_dict('records') if not fund_flow.empty else [],
                'lhb_data': lhb_data.to_dict('records') if not lhb_data.empty else [],
                'data_source': 'akshare'
            }
            
        except Exception as e:
            logging.error(f"AKShareå¢å¼ºæ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    # ======================== ä¸œæ–¹è´¢å¯ŒAPIæ•°æ®é‡‡é›† ========================
    
    def get_eastmoney_data(self):
        """è·å–ä¸œæ–¹è´¢å¯Œæ•°æ®"""
        try:
            # çƒ­é—¨æ¦‚å¿µæ¿å—æ’è¡Œ
            concept_url = "http://push2.eastmoney.com/api/qt/clist/get"
            concept_params = {
                'pn': 1,
                'pz': 50,
                'po': 1,
                'np': 1,
                'ut': 'bd1d9ddb04089700cf9c27f6f7426281',
                'fltt': 2,
                'invt': 2,
                'fid': 'f3',
                'fs': 'm:90+t:3',
                'fields': 'f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f12,f13,f14,f15,f16,f17,f18,f20,f21,f23,f24,f25,f22,f11,f62,f128,f136,f115,f152'
            }
            
            response = requests.get(concept_url, params=concept_params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                return {
                    'concept_ranking': data.get('data', {}).get('diff', []),
                    'data_source': 'eastmoney'
                }
            
        except Exception as e:
            logging.error(f"ä¸œæ–¹è´¢å¯Œæ•°æ®è·å–å¤±è´¥: {e}")
            
        return None
    
    def get_excel_data(self, date_str=None):
        """è·å–Excelæå–çš„æ•°æ®"""
        if not self.excel_extractor:
            return None
            
        if not date_str:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        try:
            # å…ˆå°è¯•ä»æ•°æ®åº“è·å–å·²æå–çš„æ•°æ®
            excel_data = self.excel_extractor.get_daily_data(date_str)
            
            if not excel_data:
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ‰§è¡Œæå–
                logging.info(f"ğŸ”„ å¼€å§‹æå– {date_str} çš„Excelæ•°æ®...")
                extraction_result = self.excel_extractor.extract_all_data(date_str)
                if extraction_result:
                    excel_data = self.excel_extractor.get_daily_data(date_str)
            
            return excel_data
            
        except Exception as e:
            logging.error(f"âŒ Excelæ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def get_market_overview(self, date_str=None):
        """è·å–å¸‚åœºé¸Ÿç°æ•°æ® - åƒç‰›å“¥å…­æ­¥æ³•ç¬¬ä¸€æ­¥ - å¤šæ•°æ®æºèåˆ"""
        if not date_str:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        # 1. ä¼˜å…ˆä½¿ç”¨Excelæ•°æ®
        excel_data = self.get_excel_data(date_str)
        if excel_data and excel_data.get('market_overview'):
            logging.info("âœ… ä½¿ç”¨Excelæ•°æ®æº - å¸‚åœºé¸Ÿç°")
            return excel_data['market_overview']
        
        # 2. å°è¯•ä½¿ç”¨Tushareæ•°æ®
        tushare_data = self.get_tushare_market_overview(date_str)
        if tushare_data:
            logging.info("âœ… ä½¿ç”¨Tushareæ•°æ®æº - å¸‚åœºé¸Ÿç°")
            return self._format_market_overview_tushare(tushare_data, date_str)
        
        # 3. ä½¿ç”¨AKShareå¢å¼ºæ•°æ®
        akshare_enhanced = self.get_akshare_enhanced_data(date_str)
        if akshare_enhanced:
            logging.info("âœ… ä½¿ç”¨AKShareå¢å¼ºæ•°æ®æº - å¸‚åœºé¸Ÿç°")
            return self._format_market_overview_akshare(akshare_enhanced, date_str)
        
        # 4. å¤‡ç”¨ï¼šä¼ ç»ŸAKShareæ•°æ®
        try:
            akshare_date = date_str.replace('-', '')
            
            # è·å–Aè‚¡ç»Ÿè®¡æ•°æ®
            market_stats = ak.stock_zh_a_spot_em()
            
            if not market_stats.empty:
                # è®¡ç®—å¸‚åœºæ¦‚è§ˆæŒ‡æ ‡
                limit_up_count = len(market_stats[market_stats['æ¶¨è·Œå¹…'] >= 9.9])
                limit_down_count = len(market_stats[market_stats['æ¶¨è·Œå¹…'] <= -9.9])
                up_count = len(market_stats[market_stats['æ¶¨è·Œå¹…'] > 0])
                down_count = len(market_stats[market_stats['æ¶¨è·Œå¹…'] < 0])
                total_volume = market_stats['æˆäº¤é¢'].sum()
                
                overview = {
                    "date": date_str,
                    "limit_up_count": limit_up_count,
                    "limit_down_count": limit_down_count,
                    "up_count": up_count,
                    "down_count": down_count,
                    "total_volume": total_volume,
                    "market_sentiment": self._calculate_market_sentiment(limit_up_count, limit_down_count),
                    "data_source": "akshare_online"
                }
                
                return overview
            
        except Exception as e:
            logging.error(f"è·å–åœ¨çº¿å¸‚åœºæ¦‚è§ˆæ•°æ®å¤±è´¥: {e}")
        
        # æœ€åä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        return self._get_mock_market_overview(date_str)
    
    def _format_market_overview_tushare(self, tushare_data, date_str):
        """æ ¼å¼åŒ–Tushareå¸‚åœºæ¦‚è§ˆæ•°æ®"""
        market_stats = tushare_data.get('market_stats', {})
        indices = tushare_data.get('indices', {})
        
        # è®¡ç®—åƒç‰›å“¥æ ¸å¿ƒæŒ‡æ ‡
        market_sentiment = self._calculate_market_sentiment_enhanced(
            tushare_data['limit_up_count'], 
            tushare_data['limit_down_count'],
            market_stats.get('up_count', 0),
            market_stats.get('down_count', 0)
        )
        
        return {
            "date": date_str,
            "limit_up_count": tushare_data['limit_up_count'],
            "limit_down_count": tushare_data['limit_down_count'],
            "up_count": market_stats.get('up_count', 0),
            "down_count": market_stats.get('down_count', 0),
            "total_volume": market_stats.get('total_amount', 0),
            "avg_pct_chg": market_stats.get('avg_pct_chg', 0),
            "indices": indices,
            "market_sentiment": market_sentiment,
            "qianniu_analysis": self._qianniu_market_analysis(market_sentiment, tushare_data),
            "data_source": "tushare_pro"
        }
    
    def _format_market_overview_akshare(self, akshare_data, date_str):
        """æ ¼å¼åŒ–AKShareå¢å¼ºæ•°æ®"""
        concept_boards = akshare_data.get('concept_boards', [])
        industry_boards = akshare_data.get('industry_boards', [])
        
        # åˆ†æçƒ­é—¨æ¿å—
        hot_concepts = sorted(concept_boards, key=lambda x: x.get('æ¶¨è·Œå¹…', 0), reverse=True)[:10]
        hot_industries = sorted(industry_boards, key=lambda x: x.get('æ¶¨è·Œå¹…', 0), reverse=True)[:10]
        
        return {
            "date": date_str,
            "hot_concepts": hot_concepts,
            "hot_industries": hot_industries,
            "fund_flow_ranking": akshare_data.get('fund_flow', [])[:20],
            "lhb_highlights": akshare_data.get('lhb_data', []),
            "market_sentiment": "åŸºäºæ¿å—è½®åŠ¨åˆ†æ",
            "qianniu_analysis": self._qianniu_sector_analysis(hot_concepts, hot_industries),
            "data_source": "akshare_enhanced"
        }
    
    def _calculate_market_sentiment_enhanced(self, limit_up, limit_down, up_count, down_count):
        """å¢å¼ºç‰ˆå¸‚åœºæƒ…ç»ªè®¡ç®—"""
        if limit_up + limit_down == 0:
            ratio = 1
        else:
            ratio = limit_up / (limit_up + limit_down) if (limit_up + limit_down) > 0 else 0.5
        
        up_down_ratio = up_count / (up_count + down_count) if (up_count + down_count) > 0 else 0.5
        
        # ç»¼åˆæƒ…ç»ªè¯„åˆ†
        sentiment_score = (ratio * 0.6 + up_down_ratio * 0.4)
        
        if sentiment_score >= 0.7:
            return "å¼ºåŠ¿ä¸Šæ¶¨"
        elif sentiment_score >= 0.6:
            return "éœ‡è¡åå¼º"
        elif sentiment_score >= 0.4:
            return "éœ‡è¡æ•´ç†"
        elif sentiment_score >= 0.3:
            return "éœ‡è¡åå¼±"
        else:
            return "å¼±åŠ¿ä¸‹è·Œ"
    
    def _qianniu_market_analysis(self, sentiment, tushare_data):
        """åƒç‰›å“¥å¸‚åœºåˆ†æ"""
        limit_up = tushare_data['limit_up_count']
        limit_down = tushare_data['limit_down_count']
        
        if limit_up > 50 and limit_down < 10:
            return "å¸‚åœºæƒ…ç»ªé«˜æ¶¨ï¼Œè¿æ¿æ•ˆåº”æ˜¾è‘—ï¼Œå¯ç§¯æå‚ä¸"
        elif limit_up > 30 and limit_down < 20:
            return "å¸‚åœºæƒ…ç»ªè‰¯å¥½ï¼Œé€‰æ‹©ä¼˜è´¨ä¸ªè‚¡å‚ä¸"
        elif limit_up < 20 and limit_down > 30:
            return "å¸‚åœºæƒ…ç»ªä½è¿·ï¼Œæ§åˆ¶ä»“ä½ï¼Œè§‚æœ›ä¸ºä¸»"
        else:
            return "å¸‚åœºæƒ…ç»ªä¸­æ€§ï¼Œå¯ç²¾é€‰ä¸ªè‚¡å‚ä¸"
    
    def _qianniu_sector_analysis(self, hot_concepts, hot_industries):
        """åƒç‰›å“¥æ¿å—åˆ†æ"""
        if not hot_concepts and not hot_industries:
            return "æ¿å—è½®åŠ¨ä¸æ˜æ˜¾ï¼Œä¸ªè‚¡è¡Œæƒ…ä¸ºä¸»"
        
        strongest_concept = hot_concepts[0] if hot_concepts else None
        strongest_industry = hot_industries[0] if hot_industries else None
        
        analysis = "å½“å‰çƒ­é—¨æ¿å—: "
        if strongest_concept:
            analysis += f"æ¦‚å¿µ-{strongest_concept.get('æ¿å—åç§°', 'N/A')} "
        if strongest_industry:
            analysis += f"è¡Œä¸š-{strongest_industry.get('æ¿å—åç§°', 'N/A')}"
        
        return analysis + "ã€‚å…³æ³¨æ¿å—å†…é¾™å¤´è‚¡çš„è¡¨ç°"
    
    def get_risk_scan_data(self, date_str=None):
        """è·å–é£é™©æ‰«ææ•°æ® - åƒç‰›å“¥å…­æ­¥æ³•ç¬¬äºŒæ­¥"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
            
        try:
            # è·å–è·Œå¹…æ¦œ
            decline_rank = ak.stock_zh_a_spot_em()
            decline_rank = decline_rank.sort_values('è·Œæ¶¨å¹…', ascending=True).head(50)
            
            # è·å–è·Œåœè‚¡
            limit_down = decline_rank[decline_rank['è·Œæ¶¨å¹…'] <= -9.9]
            
            # åˆ†ææˆäº¤é‡è¡°å‡æƒ…å†µ
            volume_analysis = self._analyze_volume_decline(decline_rank)
            
            risk_data = {
                "date": date_str,
                "decline_rank": decline_rank.to_dict('records'),
                "limit_down_stocks": limit_down.to_dict('records'),
                "volume_analysis": volume_analysis,
                "risk_sectors": self._identify_risk_sectors(decline_rank)
            }
            
            return risk_data
            
        except Exception as e:
            logging.error(f"è·å–é£é™©æ‰«ææ•°æ®å¤±è´¥: {e}")
            return {}
    
    def get_opportunity_scan_data(self, date_str=None):
        """è·å–æœºä¼šæ‰«ææ•°æ® - åƒç‰›å“¥å…­æ­¥æ³•ç¬¬ä¸‰æ­¥"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
            
        try:
            # è·å–æ¶¨å¹…æ¦œ
            gain_rank = ak.stock_zh_a_spot_em()
            gain_rank = gain_rank.sort_values('è·Œæ¶¨å¹…', ascending=False).head(100)
            
            # è·å–è¿æ¿è‚¡
            consecutive_boards = self._get_consecutive_limit_up(gain_rank)
            
            # è·å–é˜¶æ®µæ¶¨å¹…æ¦œ
            stage_gainers = self._get_stage_gainers(date_str)
            
            # é‡ä»·é½å‡åˆ†æ
            volume_price_rise = self._analyze_volume_price_rise(gain_rank)
            
            opportunity_data = {
                "date": date_str,
                "gain_rank": gain_rank.head(50).to_dict('records'),
                "consecutive_boards": consecutive_boards,
                "stage_gainers": stage_gainers,
                "volume_price_rise": volume_price_rise,
                "hot_sectors": self._identify_hot_sectors(gain_rank)
            }
            
            return opportunity_data
            
        except Exception as e:
            logging.error(f"è·å–æœºä¼šæ‰«ææ•°æ®å¤±è´¥: {e}")
            return {}
    
    def get_capital_verification_data(self, date_str=None):
        """è·å–èµ„é‡‘éªŒè¯æ•°æ® - åƒç‰›å“¥å…­æ­¥æ³•ç¬¬å››æ­¥"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
            
        try:
            # è·å–æˆäº¤é¢å‰50è‚¡ç¥¨
            volume_leaders = ak.stock_zh_a_spot_em()
            volume_leaders = volume_leaders.sort_values('æˆäº¤é¢', ascending=False).head(50)
            
            # æ¿å—Kçº¿é‡ä»·åˆ†æ
            sector_volume_analysis = self._analyze_sector_volume_price()
            
            # èµ„é‡‘æµå‘åˆ†æ
            capital_flow = self._analyze_capital_flow(date_str)
            
            verification_data = {
                "date": date_str,
                "volume_leaders": volume_leaders.to_dict('records'),
                "sector_analysis": sector_volume_analysis,
                "capital_flow": capital_flow,
                "momentum_funds": self._identify_momentum_funds(volume_leaders)
            }
            
            return verification_data
            
        except Exception as e:
            logging.error(f"è·å–èµ„é‡‘éªŒè¯æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def save_fuPan_record(self, user_id, date, step_data, predictions):
        """ä¿å­˜å¤ç›˜è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO fuPan_records 
            (user_id, date, step1_market_overview, step2_risk_scan, 
             step3_opportunity_scan, step4_capital_verification, 
             step5_logic_check, step6_portfolio_management, predictions)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            user_id, date,
            json.dumps(step_data.get('step1', {})),
            json.dumps(step_data.get('step2', {})),
            json.dumps(step_data.get('step3', {})),
            json.dumps(step_data.get('step4', {})),
            json.dumps(step_data.get('step5', {})),
            json.dumps(step_data.get('step6', {})),
            json.dumps(predictions)
        ))
        
        record_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return record_id
    
    def calculate_next_day_score(self, record_id):
        """è®¡ç®—ç¬¬äºŒå¤©éªŒè¯å¾—åˆ†"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–å¤ç›˜è®°å½•
        cursor.execute('SELECT * FROM fuPan_records WHERE id = ?', (record_id,))
        record = cursor.fetchone()
        
        if not record:
            return None
            
        predictions = json.loads(record[8])  # predictionså­—æ®µ
        
        # è·å–ç¬¬äºŒå¤©å®é™…è¡¨ç°
        next_day = (datetime.strptime(record[2], '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')
        actual_performance = self._get_actual_performance(next_day, predictions)
        
        # è®¡ç®—é¢„æµ‹å‡†ç¡®åº¦å¾—åˆ†
        prediction_score = self._calculate_prediction_accuracy(predictions, actual_performance)
        
        # è®¡ç®—åˆ†ææ·±åº¦å¾—åˆ†  
        depth_score = self._calculate_analysis_depth(record)
        
        total_score = prediction_score + depth_score
        
        # ä¿å­˜è¯„åˆ†ç»“æœ
        cursor.execute('''
            INSERT INTO scoring_results 
            (record_id, prediction_accuracy_score, analysis_depth_score, 
             total_score, next_day_performance, scoring_date)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            record_id, prediction_score, depth_score, total_score,
            json.dumps(actual_performance), next_day
        ))
        
        conn.commit()
        conn.close()
        
        return {
            "prediction_score": prediction_score,
            "depth_score": depth_score,
            "total_score": total_score,
            "actual_performance": actual_performance
        }
    
    def _get_mock_market_overview(self, date_str):
        """è·å–æ¨¡æ‹Ÿå¸‚åœºæ¦‚è§ˆæ•°æ®"""
        return {
            "date": date_str,
            "limit_up_count": 45,
            "limit_down_count": 8,
            "up_count": 2850,
            "down_count": 1650,
            "total_volume": 1.15e12,
            "market_sentiment": "éœ‡è¡åå¼º",
            "qianniu_analysis": "å¸‚åœºæƒ…ç»ªä¸­æ€§ï¼Œå¯ç²¾é€‰ä¸ªè‚¡å‚ä¸",
            "data_source": "mock_data"
        }
    
    def _calculate_market_sentiment(self, limit_up_count, limit_down_count):
        """è®¡ç®—å¸‚åœºæƒ…ç»ª - åƒç‰›å“¥åˆ†ææ³•"""
        if limit_up_count > 80:
            return "å¼ºåŠ¿ä¸Šæ¶¨"
        elif limit_up_count > 50:
            return "éœ‡è¡åå¼º"
        elif limit_up_count > 20:
            return "éœ‡è¡"
        elif limit_down_count > 50:
            return "å¼±åŠ¿ä¸‹è·Œ"
        else:
            return "éœ‡è¡åå¼±"
    
    def _analyze_volume_decline(self, decline_data):
        """åˆ†ææˆäº¤é‡è¡°å‡"""
        return {"volume_decline_ratio": 0.15, "risk_level": "ä¸­ç­‰"}
    
    def _identify_risk_sectors(self, decline_data):
        """è¯†åˆ«é«˜é£é™©æ¿å—"""
        return ["æˆ¿åœ°äº§", "é“¶è¡Œ"]
    
    def _get_consecutive_limit_up(self, gain_data):
        """è·å–è¿æ¿è‚¡"""
        return gain_data[gain_data['è·Œæ¶¨å¹…'] >= 9.9].to_dict('records')
    
    def _get_stage_gainers(self, date_str):
        """è·å–é˜¶æ®µæ¶¨å¹…æ¦œ"""
        return []
    
    def _analyze_volume_price_rise(self, gain_data):
        """åˆ†æé‡ä»·é½å‡"""
        return {"strong_stocks": [], "volume_price_correlation": 0.8}
    
    def _identify_hot_sectors(self, gain_data):
        """è¯†åˆ«çƒ­é—¨æ¿å—"""
        return ["æ–°èƒ½æº", "äººå·¥æ™ºèƒ½"]
    
    def _analyze_sector_volume_price(self):
        """åˆ†ææ¿å—é‡ä»·å…³ç³»"""
        return {"sector_momentum": {}}
    
    def _analyze_capital_flow(self, date_str):
        """åˆ†æèµ„é‡‘æµå‘"""
        return {"net_inflow": 1000000000, "main_inflow_sectors": []}
    
    def _identify_momentum_funds(self, volume_data):
        """è¯†åˆ«åŠ¨é‡èµ„é‡‘"""
        return {"momentum_ratio": 0.6, "smart_money_flow": "æµå…¥"}
    
    def _get_actual_performance(self, date, predictions):
        """è·å–å®é™…è¡¨ç°æ•°æ®"""
        # å®ç°è·å–æ¬¡æ—¥å®é™…è¡¨ç°çš„é€»è¾‘
        return {"sector_performance": {}, "stock_performance": {}}
    
    def _calculate_prediction_accuracy(self, predictions, actual):
        """è®¡ç®—é¢„æµ‹å‡†ç¡®åº¦"""
        # å®ç°é¢„æµ‹å‡†ç¡®åº¦è®¡ç®—é€»è¾‘
        return 50.0  # æ»¡åˆ†70åˆ†
    
    def _calculate_analysis_depth(self, record):
        """è®¡ç®—åˆ†ææ·±åº¦å¾—åˆ†"""
        # å®ç°åˆ†ææ·±åº¦è¯„åˆ†é€»è¾‘
        return 25.0  # æ»¡åˆ†30åˆ†

if __name__ == "__main__":
    # æµ‹è¯•ç³»ç»Ÿ
    system = StockDataSystem()
    
    # æµ‹è¯•æ•°æ®è·å–
    overview = system.get_market_overview()
    print("å¸‚åœºæ¦‚è§ˆæ•°æ®è·å–æˆåŠŸ")
    
    risk_data = system.get_risk_scan_data()
    print("é£é™©æ‰«ææ•°æ®è·å–æˆåŠŸ")
    
    opportunity_data = system.get_opportunity_scan_data()
    print("æœºä¼šæ‰«ææ•°æ®è·å–æˆåŠŸ")